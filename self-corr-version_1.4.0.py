'''
----------------------------- ä»£ç ç”¨é€” ---------------------------
ä½œè€…ï¼šå°æˆ¿æ€»
è¯¥ä»£ç åœ¨é¡¾é—®â€œåå­å“¥â€çš„æºä»£ç åŸºç¡€ä¸Šï¼Œå¢åŠ äº†æ‰¹é‡æ£€æµ‹ï¼Œä»¥åŠsharpeå€¼æ£€éªŒçš„åŠŸèƒ½ï¼Œ
ç›®å‰å·²ç»èƒ½å¤Ÿæ»¡è¶³Useré˜¶æ®µçš„è‡ªç›¸å…³æ£€æµ‹ï¼Œæ‰¹é‡æ£€æŸ¥å› å­èƒ½å¦æäº¤ï¼ŒèŠ‚çœå¤§é‡æ—¶é—´ã€‚
----------------------------- ä½¿ç”¨è¯´æ˜ ---------------------------
ä½ éœ€è¦åšçš„åªæœ‰ä¸¤ç‚¹
1. åŒæ–‡ä»¶å¤¹ä¸‹åˆ›å»ºåä¸ºbrain_credentials.txtçš„æ–‡ä»¶ï¼Œ
é‡Œé¢çš„æ ¼å¼ä¸ºï¼š["è´¦å·", "å¯†ç "]
2. å°†ALPHA_LISTé‡Œé¢çš„å€¼æ›¿æ¢æˆä½ éœ€è¦æ£€æµ‹çš„id
3. ç»ˆç«¯é‡Œè¿è¡Œpython3 self-corr-version_1.4.0.py å³å¯
-----------------------------------------------------------------
Updatedï¼šOct 12, 2025
Version 1.1.0ç‰ˆï¼Œæ–°å¢äº†æ¨èæå‡alphaçš„åŠŸèƒ½
æ¨èæå‡çš„alphaï¼Œé€‚å½“ä¿®æ”¹ä¸€ä¸‹å› å­çš„å‚æ•°ï¼Œå¾ˆå®¹æ˜“æ•‘æ´»è¿™äº›â€œæ­»æ‰â€çš„å› å­
-----------------------------------------------------------------
Updatedï¼šOct 13, 2025
Version 1.2.0ç‰ˆï¼Œå°†alpha_listå’Œcsvæ–‡ä»¶æå‡ä¸ºå…¨å±€å˜é‡
-----------------------------------------------------------------
Updatedï¼šOct 17, 2025
Version 1.3.0ç‰ˆï¼Œcsvæ–‡ä»¶æ˜¾ç¤ºFailçš„alphaèƒ½å¦æå‡
-----------------------------------------------------------------
Updatedï¼šOct 28, 2025
Version 1.4.0ç‰ˆï¼Œæ§åˆ¶å°è¾“å‡ºæ ¼å¼ä¼˜åŒ–
'''


import requests
import pandas as pd
import logging
import time
from typing import Optional, Tuple, Dict, List
from concurrent.futures import ThreadPoolExecutor
import pickle
from collections import defaultdict
import numpy as np
from pathlib import Path
import json
from os.path import expanduser
from requests.auth import HTTPBasicAuth

# ---------------- å…¨å±€å‚æ•° ----------------
CORR_CUTOFF = 0.7         # ç›¸å…³æ€§é˜ˆå€¼ï¼š<=0.7å¿…Passï¼›>0.7è§¦å‘Sharpeå¯¹æ¯”
SHARPE_PREMIUM = 1.10     # è¢«æµ‹Sharpeè‡³å°‘éœ€é«˜å‡ºâ€œç›¸å…³peerä¸­æœ€å¤§Sharpeâ€10%
CSV_FILE = "Test.csv"
ALPHA_LIST = [
               "XgGje9b1", "kqdqj1xL", "88b8Jdlq"
             ]

# ---------------- ç™»å½• ----------------
def sign_in(username, password):
    s = requests.Session()
    s.auth = (username, password)
    try:
        response = s.post('https://api.worldquantbrain.com/authentication')
        response.raise_for_status()
        logging.info("Successfully signed in")
        return s
    except requests.exceptions.RequestException as e:
        logging.error(f"Login failed: {e}")
        return None

# ----------- ä»æ–‡ä»¶è¯»å–è´¦å·å¯†ç ç™»å½• -----------
def sign_in_from_file():
    cred_path = expanduser('brain_credentials.txt')
    try:
        with open(cred_path) as f:
            credentials = json.load(f)
        username, password = credentials
        sess = requests.Session()
        sess.auth = HTTPBasicAuth(username, password)
        response = sess.post('https://api.worldquantbrain.com/authentication')
        response.raise_for_status()
        print("âœ… ç™»å½•æˆåŠŸï¼ˆå‡­è¯æ–‡ä»¶ï¼‰")
        return sess
    except FileNotFoundError:
        print("âš ï¸ æœªæ‰¾åˆ° brain_credentials.txt æ–‡ä»¶ï¼Œå°†ä½¿ç”¨æ‰‹åŠ¨ç™»å½•æ–¹å¼ã€‚")
        return None
    except Exception as e:
        print(f"âŒ ä»æ–‡ä»¶ç™»å½•å¤±è´¥ï¼š{e}")
        return None

# ---------------- æ–‡ä»¶æ“ä½œ ----------------
def save_obj(obj: object, name: str) -> None:
    with open(name + '.pickle', 'wb') as f:
        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)

def load_obj(name: str) -> object:
    with open(name + '.pickle', 'rb') as f:
        return pickle.load(f)

# ---------------- è¯·æ±‚é‡è¯• ----------------
def wait_get(url: str, max_retries: int = 10):
    retries = 0
    while retries < max_retries:
        while True:
            simulation_progress = sess.get(url)
            if simulation_progress.headers.get("Retry-After", 0) == 0:
                break
            time.sleep(float(simulation_progress.headers["Retry-After"]))
        if simulation_progress.status_code < 400:
            break
        else:
            time.sleep(2 ** retries)
            retries += 1
    return simulation_progress

# ---------------- è·å–å•ä¸ª Alpha PnL ----------------
def _get_alpha_pnl(alpha_id: str) -> pd.DataFrame:
    pnl = wait_get("https://api.worldquantbrain.com/alphas/" + alpha_id + "/recordsets/pnl").json()
    df = pd.DataFrame(pnl['records'], columns=[item['name'] for item in pnl['schema']['properties']])
    df = df.rename(columns={'date':'Date', 'pnl':alpha_id})
    df = df[['Date', alpha_id]]
    return df

# ---------------- æ‰¹é‡è·å– PnL ----------------
def get_alpha_pnls(alphas: list[dict],
                   alpha_pnls: Optional[pd.DataFrame] = None,
                   alpha_ids: Optional[dict[str, list]] = None) -> Tuple[dict[str, list], pd.DataFrame]:
    if alpha_ids is None:
        alpha_ids = defaultdict(list)
    if alpha_pnls is None:
        alpha_pnls = pd.DataFrame()

    new_alphas = [item for item in alphas if item['id'] not in alpha_pnls.columns]
    if not new_alphas:
        return alpha_ids, alpha_pnls

    for item_alpha in new_alphas:
        alpha_ids[item_alpha['settings']['region']].append(item_alpha['id'])

    fetch_pnl_func = lambda alpha_id: _get_alpha_pnl(alpha_id).set_index('Date')
    with ThreadPoolExecutor(max_workers=10) as executor:
        results = executor.map(fetch_pnl_func, [item['id'] for item in new_alphas])
    alpha_pnls = pd.concat([alpha_pnls] + list(results), axis=1)
    alpha_pnls.sort_index(inplace=True)
    return alpha_ids, alpha_pnls

# ---------------- è·å– OS Alpha åˆ—è¡¨ ----------------
def get_os_alphas(limit: int = 100, get_first: bool = False) -> List[Dict]:
    fetched_alphas = []
    offset = 0
    total_alphas = 100
    while len(fetched_alphas) < total_alphas:
        print(f"Fetching alphas from offset {offset} to {offset + limit} ...")
        url = f"https://api.worldquantbrain.com/users/self/alphas?stage=OS&limit={limit}&offset={offset}&order=-dateSubmitted"
        res = wait_get(url).json()
        if offset == 0:
            total_alphas = res['count']
            print(f"ğŸ” å…±å‘ç° {total_alphas} æ¡ OS alphaï¼Œå‡†å¤‡è·å–æœ€æ–° {limit if get_first else total_alphas} æ¡...")
        alphas = res["results"]
        fetched_alphas.extend(alphas)
        print(f"âœ… æœ¬æ¬¡å·²è·å– {len(fetched_alphas)} æ¡")

        if len(alphas) < limit or get_first:
            break
        offset += limit
    print(f"âœ… OS alpha åˆ—è¡¨è·å–å®Œæˆï¼Œå…±è¿”å› {len(fetched_alphas)} æ¡")
    print("æ­£åœ¨ä¸‹è½½ï¼Œè¯·è€å¿ƒç­‰å¾…...")
    return fetched_alphas[:total_alphas]

# ---------------- è·å– Sharpe å€¼ï¼ˆå¸¦ç®€å•è¿è¡ŒæœŸç¼“å­˜ï¼‰ ----------------
_sharpe_cache_runtime: Dict[str, float] = {}

def get_alpha_sharpe(alpha_id: str) -> float:
    """ä» API è·å–å•ä¸ª alpha çš„ Sharpe å€¼ï¼›è¿è¡ŒæœŸç¼“å­˜é¿å…é‡å¤IO"""
    if alpha_id in _sharpe_cache_runtime:
        return _sharpe_cache_runtime[alpha_id]
    try:
        data = wait_get(f"https://api.worldquantbrain.com/alphas/{alpha_id}").json()
        checks = data.get("is", {}).get("checks", [])
        match = next((c for c in checks if c.get("name") == "LOW_SHARPE"), None)
        if match and "value" in match:
            val = float(match["value"])
        elif match and "result" in match and isinstance(match["result"], (int, float)):
            val = float(match["result"])
        else:
            val = np.nan
    except Exception as e:
        print(f"âš ï¸ è·å– {alpha_id} çš„ Sharpe å€¼å¤±è´¥: {e}")
        val = np.nan
    _sharpe_cache_runtime[alpha_id] = val
    return val

# ---------------- è®¡ç®—å•ä¸ª Alpha è‡ªç›¸å…³ï¼ˆè¿”å›å…¨åºåˆ—ï¼‰ ----------------
def calc_self_corr_series(alpha_id: str,
                          os_alpha_rets: pd.DataFrame | None = None,
                          os_alpha_ids: dict[str, str] | None = None,
                          alpha_result: dict | None = None,
                          alpha_pnls: pd.DataFrame | None = None) -> pd.Series:
    if alpha_result is None:
        alpha_result = wait_get(f"https://api.worldquantbrain.com/alphas/{alpha_id}").json()
    if alpha_pnls is not None and len(alpha_pnls) == 0:
        alpha_pnls = None
    if alpha_pnls is None:
        _, alpha_pnls = get_alpha_pnls([alpha_result])
        alpha_pnls = alpha_pnls[alpha_id]
    alpha_rets = alpha_pnls - alpha_pnls.ffill().shift(1)
    alpha_rets = alpha_rets[pd.to_datetime(alpha_rets.index) > pd.to_datetime(alpha_rets.index).max() - pd.DateOffset(years=4)]
    region = alpha_result['settings']['region']
    pool = os_alpha_rets[os_alpha_ids[region]]
    corr_series = pool.corrwith(alpha_rets).sort_values(ascending=False).round(4)
    return corr_series

# ---------------- ä¸‹è½½æ•°æ® ----------------
def download_data(flag_increment=True):
    if flag_increment:
        try:
            os_alpha_ids = load_obj(str(cfg.data_path / 'os_alpha_ids'))
            os_alpha_pnls = load_obj(str(cfg.data_path / 'os_alpha_pnls'))
            ppac_alpha_ids = load_obj(str(cfg.data_path / 'ppac_alpha_ids'))
            exist_alpha = [alpha for ids in os_alpha_ids.values() for alpha in ids]
        except Exception:
            os_alpha_ids = None
            os_alpha_pnls = None
            exist_alpha = []
            ppac_alpha_ids = []
    else:
        os_alpha_ids = None
        os_alpha_pnls = None
        exist_alpha = []
        ppac_alpha_ids = []
    alphas = get_os_alphas(limit=100, get_first=False)

    alphas = [item for item in alphas if item['id'] not in exist_alpha]
    ppac_alpha_ids += [item['id'] for item in alphas for item_match in item['classifications'] if item_match['name'] == 'Power Pool Alpha']
    os_alpha_ids, os_alpha_pnls = get_alpha_pnls(alphas, alpha_pnls=os_alpha_pnls, alpha_ids=os_alpha_ids)
    save_obj(os_alpha_ids, str(cfg.data_path / 'os_alpha_ids'))
    save_obj(os_alpha_pnls, str(cfg.data_path / 'os_alpha_pnls'))
    save_obj(ppac_alpha_ids, str(cfg.data_path / 'ppac_alpha_ids'))
    print(f'æ–°ä¸‹è½½çš„alphaæ•°é‡: {len(alphas)}, ç›®å‰æ€»å…±æäº¤alphaæ•°é‡: {os_alpha_pnls.shape[1]}')

# ---------------- åŠ è½½æ•°æ® ----------------
def load_data(tag=None):
    os_alpha_ids = load_obj(str(cfg.data_path / 'os_alpha_ids'))
    os_alpha_pnls = load_obj(str(cfg.data_path / 'os_alpha_pnls'))
    ppac_alpha_ids = load_obj(str(cfg.data_path / 'ppac_alpha_ids'))
    if tag == 'PPAC':
        for item in os_alpha_ids:
            os_alpha_ids[item] = [alpha for alpha in os_alpha_ids[item] if alpha in ppac_alpha_ids]
    elif tag == 'SelfCorr':
        for item in os_alpha_ids:
            os_alpha_ids[item] = [alpha for alpha in os_alpha_ids[item] if alpha not in ppac_alpha_ids]
    exist_alpha = [alpha for ids in os_alpha_ids.values() for alpha in ids]
    os_alpha_pnls = os_alpha_pnls[exist_alpha]
    os_alpha_rets = os_alpha_pnls - os_alpha_pnls.ffill().shift(1)
    os_alpha_rets = os_alpha_rets[pd.to_datetime(os_alpha_rets.index) > pd.to_datetime(os_alpha_rets.index).max() - pd.DateOffset(years=4)]
    return os_alpha_ids, os_alpha_rets

# ---------------- é…ç½®ç±» ----------------
class cfg:
    username = ""
    password = ""
    data_path = Path('.')

# ---------------- ä¸»ç¨‹åº ----------------
if __name__ == "__main__":
    sess = sign_in_from_file()
    if sess is None:
        sess = sign_in(cfg.username, cfg.password)

    download_data(flag_increment=True)
    os_alpha_ids, os_alpha_rets = load_data()

    print(f"å³å°†æµ‹è¯•çš„alphaæ•°é‡ä¸º {len(ALPHA_LIST)} æ¡")

    results = {}
    for idx, alpha_id in enumerate(ALPHA_LIST, start=1):
        try:
            corr_series = calc_self_corr_series(alpha_id, os_alpha_rets=os_alpha_rets, os_alpha_ids=os_alpha_ids)
            max_corr = float(corr_series.max()) if not corr_series.empty else 0.0
            max_corr_id = str(corr_series.idxmax()) if not corr_series.empty else None
            sharpe_current = get_alpha_sharpe(alpha_id)

            status = "Pass"
            peer_ids_over = corr_series[corr_series > CORR_CUTOFF].index.tolist()
            max_peer_sharpe = np.nan
            if len(peer_ids_over) > 0:
                peer_sharpes = [get_alpha_sharpe(pid) for pid in peer_ids_over if not np.isnan(get_alpha_sharpe(pid))]
                if len(peer_sharpes) == 0:
                    status = "Fail (Peers>0.7 but Sharpe missing)"
                else:
                    max_peer_sharpe = max(peer_sharpes)
                    if np.isnan(sharpe_current) or sharpe_current < SHARPE_PREMIUM * max_peer_sharpe:
                        status = "Fail"
                    else:
                        status = "Pass"
            else:
                status = "Pass"

            results[alpha_id] = {
                "Corr_Max": max_corr,
                "Corr_Max_ID": max_corr_id,
                "Corr_Cutoff": CORR_CUTOFF,
                "Num_Peers_Over_Cutoff": len(peer_ids_over),
                "Sharpe_Current": sharpe_current,
                "Sharpe_Peers_MaxOverCutoff": max_peer_sharpe,
                "Sharpe_Premium": SHARPE_PREMIUM,
                "Result": status,
                "Wait_to_Approve": False
            }

            if len(peer_ids_over) == 0:
                print(f"{idx}. {alpha_id} - {status} | MaxCorr={max_corr:.4f} (<= {CORR_CUTOFF}), Sharpe={sharpe_current:.3f}")
            else:
                print(f"{idx}. {alpha_id} - {status} | MaxCorr={max_corr:.4f} (> {CORR_CUTOFF}), "
                      f"Sharpe={sharpe_current:.3f}, MaxPeerSharpe={max_peer_sharpe if not np.isnan(max_peer_sharpe) else float('nan'):.3f}, "
                      f"PeersOver={len(peer_ids_over)}")

        except Exception as e:
            results[alpha_id] = {"Result": f"Error - {str(e)}"}
            print(f"{idx}. {alpha_id}: Error - {e}")

    # æ±‡æ€»è¾“å‡º
    total = len(results)
    pass_ids = [k for k, v in results.items() if v.get("Result") == "Pass"]
    fail_count = total - len(pass_ids)
    print("\n" + "=" * 100)
    print(f"Resultï¼šå…±æ‰§è¡Œ {total} æ¡è®°å½•ã€‚Pass = {len(pass_ids)}ï¼ŒFail = {fail_count}")

    if len(pass_ids) > 0:
        print("é€šè¿‡çš„ Alpha IDï¼š")
        # ä¸€è¡Œè¾“å‡ºï¼Œå¸¦å¼•å·
        print("  " + ", ".join(f'"{x}"' for x in pass_ids))
    else:
        print("æ²¡æœ‰é€šè¿‡çš„ Alphaã€‚")

    print("=" * 100)

    # ---------------- æ¨èæå‡ Alpha ----------------
    recommend_ids = []
    for k, v in results.items():
        if (
            v.get("Result") == "Fail"
            and v.get("Num_Peers_Over_Cutoff") == 1
            and not np.isnan(v.get("Sharpe_Peers_MaxOverCutoff"))
            and v.get("Sharpe_Peers_MaxOverCutoff") > 0
            and v.get("Sharpe_Current", 0) / v.get("Sharpe_Peers_MaxOverCutoff") > 1.05
        ):
            recommend_ids.append(k)
            results[k]["Wait_to_Approve"] = True

    print("\n" + "=" * 100)
    print(f"æ¨èæå‡Alphaï¼šå…±{len(recommend_ids)}æ¡è®°å½•")
    if recommend_ids:
        print("Alpha IDï¼š")
        # ä¸€è¡Œè¾“å‡ºï¼Œå¸¦å¼•å·
        print("  " + ", ".join(f'"{x}"' for x in recommend_ids))
    else:
        print("æš‚æ— ç¬¦åˆæ¡ä»¶çš„æ¨èAlphaã€‚")
    print("=" * 100)

    # è¾“å‡º CSV
    result_df = pd.DataFrame([{"Alpha_ID": k, **v} for k, v in results.items()])
    result_df.to_csv(f"{CSV_FILE}", index=False)

    print(f"\næ£€æµ‹å®Œæˆï¼Œç»“æœå·²ä¿å­˜åˆ° {CSV_FILE} âœ…")
